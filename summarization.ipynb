{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFaefv4_KnJn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import textwrap\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial import distance\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-a3QU3KKojT",
        "outputId": "fde400da-c801-43e5-f68e-fe2fb1afa2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acRXSAP4LKA5",
        "outputId": "9322dfc7-f43b-45d3-ab27-e072ebbe041f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m92.2/97.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.7.22)\n",
            "Building wheels for collected packages: breadability, docopt, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=07fd371879e12deb4be5b0447a41a8730b27e0e441a9f3dd18ac96580e2c8943\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=611f2f8b4a60264afa3a88cda952a2acf71618a24ddc7ceb62e7f9e479188716\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681831 sha256=10f63e8d60cb9857b181efe7a1601fa7da22edd1b036cc4d5d6d28702564faa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/57/cc/290c5252ec97a6d78d36479a3c5e5ecc76318afcb241ad9dbe\n",
            "Successfully built breadability docopt pycountry\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/bbc_text_cls.csv', 'r') as f:\n",
        "    content = f.read()\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJOnIwXLLV3n",
        "outputId": "3b7b8481-940a-4a23-c616-e063cce8e6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/My Drive/bbc_text_cls.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIawU7S4NNOm",
        "outputId": "9b54db3b-dc25-4c33-ebcd-d6bb2edeae50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text    labels\n",
            "0     Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
            "1     Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
            "2     Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
            "3     High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
            "4     Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
            "...                                                 ...       ...\n",
            "2220  BT program to beat dialler scams\\n\\nBT is intr...      tech\n",
            "2221  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech\n",
            "2222  Be careful how you code\\n\\nA new European dire...      tech\n",
            "2223  US cyber security chief resigns\\n\\nThe man mak...      tech\n",
            "2224  Losing yourself in online gaming\\n\\nOnline rol...      tech\n",
            "\n",
            "[2225 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = df[df.labels == 'business']['text'].sample(random_state=24)\n"
      ],
      "metadata": {
        "id": "_9vxjnvqOFxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YktfwKHGPFUE",
        "outputId": "f904db9b-92c7-47f6-b19b-754a200fed2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370    Singapore growth at 8.1% in 2004\\n\\nSingapore'...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EWHn9g4MOiZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IPz9Sy-YOHYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap(x):\n",
        "    return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n",
        "\n",
        "input_text = \"This is a long piece of text that needs to be wrapped properly in a formatted way. It should consider sentence endings and not replace existing whitespace.\"\n",
        "\n",
        "wrapped_text = wrap(input_text)\n",
        "print(wrapped_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29-MlufZOjsU",
        "outputId": "f89e829d-4e3a-47ee-cdb4-ce34b983835a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a long piece of text that needs to be wrapped properly in a\n",
            "formatted way.  It should consider sentence endings and not replace\n",
            "existing whitespace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap(x):\n",
        "  return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "5oSRx93jOjx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrap(doc.iloc[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLjoQCBdO8Sa",
        "outputId": "11063050-62de-4fe7-dfee-e6839bf4dbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Singapore growth at 8.1% in 2004\n",
            "\n",
            "Singapore's economy grew by 8.1% in\n",
            "2004, its best performance since 2000, figures from the trade ministry\n",
            "show.\n",
            "\n",
            "The advance, the second-fastest in Asia after China, was led by\n",
            "growth of 13.1% in the key manufacturing sector.  However, a slower-\n",
            "than-expected fourth quarter points to more modest growth for the\n",
            "trade-driven economy in 2005 as global technology demand falls back.\n",
            "Slowdowns in the US and China could hit electronics exports, while the\n",
            "tsunami disaster may effect the service sector.\n",
            "\n",
            "Economic growth is\n",
            "set to halve in Singapore this year to between 3% and 5%. In the\n",
            "fourth quarter, the city state's gross domestic product (GDP) rose at\n",
            "an annual rate of 2.4%. That was up from the third quarter, when it\n",
            "fell 3.0%, but was well below analyst forecasts.  \"I am surprised at\n",
            "the weak fourth quarter number.  The main drag came from electronics,\"\n",
            "said Lian Chia Liang, economist at JP Morgan Chase.  Singapore's\n",
            "economy had contracted over the summer, weighed down by soaring oil\n",
            "prices.  The economy's poor performance in the July to September\n",
            "period followed four consecutive quarters of double-digit growth as\n",
            "Singapore bounced back strongly from the effects of the deadly Sars\n",
            "virus in 2003.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u3rmzqnzPflg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "summarizer = TextRankSummarizer()\n",
        "parser = PlaintextParser.from_string(\n",
        "    doc.iloc[0].split(\"\\n\", 1)[1],\n",
        "    Tokenizer(\"russian\"))\n",
        "summary = summarizer(parser.document, sentences_count=3)\n",
        "sum_=str()\n",
        "for s in summary:\n",
        "  sum_+=str(s)\n",
        "sum_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "s4XkeV06P9sr",
        "outputId": "c3cc75a5-cc09-4a55-d773-0cd829dd9e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The advance, the second-fastest in Asia after China, was led by growth of 13.1% in the key manufacturing sector.Slowdowns in the US and China could hit electronics exports, while the tsunami disaster may effect the service sector.The economy's poor performance in the July to September period followed four consecutive quarters of double-digit growth as Singapore bounced back strongly from the effects of the deadly Sars virus in 2003.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "featurizer = TfidfVectorizer(stop_words=stopwords.words('english'),norm='l1')\n",
        "# mean of tfidf score for each sencence\n",
        "def get_sentence_score(tfidf_row):\n",
        "  x = tfidf_row[tfidf_row != 0]\n",
        "  return x.mean()\n",
        "\n",
        "def summarize_tfidf(text):\n",
        "  # tokenize sentences\n",
        "  sents = nltk.sent_tokenize(text)\n",
        "  # tf-idf score\n",
        "  X = featurizer.fit_transform(sents)\n",
        "  # scores for each sentence\n",
        "  scores = np.zeros(len(sents))\n",
        "  for i in range(len(sents)):\n",
        "    score = get_sentence_score(X[i,:])\n",
        "    scores[i] = score\n",
        "  # sort the scores and pick top 5 sentences\n",
        "  sort_idx = np.argsort(-scores)\n",
        "  sort_idx=sort_idx[:5]\n",
        "  sort_idx=np.sort(sort_idx)\n",
        "  # generate summary\n",
        "  sum_=str()\n",
        "  for i in sort_idx[:5]:\n",
        "    sum_+=sents[i]\n",
        "  return sum_\n",
        "  sum_\n",
        "  print(sum_)"
      ],
      "metadata": {
        "id": "7BWdgcNeSqdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzirgcoOVRrX",
        "outputId": "28ea61be-e168-430c-bf0f-3674e3d4a05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The advance, the second-fastest in Asia after China, was led by growth of 13.1% in the key manufacturing sector.Slowdowns in the US and China could hit electronics exports, while the tsunami disaster may effect the service sector.The economy's poor performance in the July to September period followed four consecutive quarters of double-digit growth as Singapore bounced back strongly from the effects of the deadly Sars virus in 2003.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_tfidf=summarize_tfidf(doc.iloc[0].split(\"\\n\", 1)[1])\n"
      ],
      "metadata": {
        "id": "Ka4iozVBVfAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrap(sum_tfidf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxGhIO9TVltt",
        "outputId": "237a7e6c-b796-4498-954e-b3c1af63e7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The advance, the second-fastest in Asia after China, was led by growth\n",
            "of 13.1% in the key manufacturing sector.Economic growth is set to\n",
            "halve in Singapore this year to between 3% and 5%.That was up from the\n",
            "third quarter, when it fell 3.0%, but was well below analyst\n",
            "forecasts.\"I am surprised at the weak fourth quarter\n",
            "number.Singapore's economy had contracted over the summer, weighed\n",
            "down by soaring oil prices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgxqXI3jzuUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COSINE SIMILARITY STARTS SUMMARIZATION ALGO-2\n"
      ],
      "metadata": {
        "id": "-Lm-MicWzxjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def summarize_cosine(text, factor = 0.15):\n",
        "  # tokenize sentences\n",
        "  sents = nltk.sent_tokenize(text)\n",
        "  # calulate tf-idf score\n",
        "  featurizer = TfidfVectorizer(stop_words=stopwords.words('english'),norm='l1')\n",
        "  X = featurizer.fit_transform(sents)\n",
        "  # compute similarity matrix\n",
        "  S = cosine_similarity(X)\n",
        "  # normalize similarity matrix\n",
        "  S /= S.sum(axis=1, keepdims=True)\n",
        "  # uniform transition matrix\n",
        "  U = np.ones_like(S) / len(S)\n",
        "  # smoothed similarity matrix\n",
        "  S = (1 - factor) * S + factor * U\n",
        "  # find the limiting / stationary distribution\n",
        "  eigenvals, eigenvecs = np.linalg.eig(S.T)\n",
        "  # compute scores\n",
        "  scores = eigenvecs[:,0] / eigenvecs[:,0].sum()\n",
        "  # sort the scores and pick top 5 sentences\n",
        "  sort_idx = np.argsort(-scores)\n",
        "  sort_idx=sort_idx[:5]\n",
        "  sort_idx=np.sort(sort_idx)\n",
        "  # generate summary\n",
        "  sum_=str()\n",
        "  for i in sort_idx[:5]:\n",
        "    #print(wrap(\"%.2f: %s\" % (scores[i], sents[i])))\n",
        "    sum_+=sents[i]\n",
        "  return sum_"
      ],
      "metadata": {
        "id": "OV8E9l1fz4Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_cos=summarize_cosine(doc.iloc[0].split(\"\\n\", 1)[1], factor = 0.15)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uJDuPWVQ1HC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrap(sum_cos))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJdTrgFe1nNx",
        "outputId": "76b58275-4951-4a5f-e480-1a9a5005c990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Singapore's economy grew by 8.1% in 2004, its best performance since\n",
            "2000, figures from the trade ministry show.However, a slower-than-\n",
            "expected fourth quarter points to more modest growth for the trade-\n",
            "driven economy in 2005 as global technology demand falls back.Economic\n",
            "growth is set to halve in Singapore this year to between 3% and 5%.\"I\n",
            "am surprised at the weak fourth quarter number.The economy's poor\n",
            "performance in the July to September period followed four consecutive\n",
            "quarters of double-digit growth as Singapore bounced back strongly\n",
            "from the effects of the deadly Sars virus in 2003.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWCeWPgO1ocS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K MEANS CLUSTERING everything about"
      ],
      "metadata": {
        "id": "4Adx2DA_1q9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = nltk.sent_tokenize(doc.iloc[0].split(\"\\n\", 1)[1])\n",
        "corpus = []\n",
        "for i in range(len(sentence)):\n",
        "    sen = re.sub('[^a-zA-Z]', \" \", sentence[i])\n",
        "    sen = sen.lower()\n",
        "    sen = sen.split()\n",
        "    sen = ' '.join([i for i in sen if i not in stopwords.words('english')])\n",
        "    corpus.append(sen)\n",
        "\n",
        "all_words = [i.split() for i in corpus]\n",
        "model = Word2Vec(all_words, min_count=1,vector_size=300)\n",
        "\n",
        "sent_vector=[]\n",
        "for i in corpus:\n",
        "    plus=0\n",
        "    for j in i.split():\n",
        "        plus+= model.wv[j]\n",
        "    plus = plus/len(i.split())\n",
        "    sent_vector.append(plus)\n",
        "\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(sent_vector)\n",
        "\n",
        "my_list=[]\n",
        "for i in range(n_clusters):\n",
        "    my_dict={}\n",
        "    for j in range(len(y_kmeans)):\n",
        "        if y_kmeans[j]==i:\n",
        "            my_dict[j] =  distance.euclidean(kmeans.cluster_centers_[i],sent_vector[j])\n",
        "    min_distance = min(my_dict.values())\n",
        "    my_list.append(min(my_dict, key=my_dict.get))\n",
        "sum_kmean=str()\n",
        "for i in sorted(my_list):\n",
        "    sum_kmean+=sentence[i]\n",
        "\n",
        "print(wrap(sum_kmean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0y4oYZL1xzn",
        "outputId": "e05bde03-32cc-4581-f3a8-14e14e10f1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Singapore's economy grew by 8.1% in 2004, its best performance since\n",
            "2000, figures from the trade ministry show.Economic growth is set to\n",
            "halve in Singapore this year to between 3% and 5%.That was up from the\n",
            "third quarter, when it fell 3.0%, but was well below analyst\n",
            "forecasts.\"I am surprised at the weak fourth quarter number.The\n",
            "economy's poor performance in the July to September period followed\n",
            "four consecutive quarters of double-digit growth as Singapore bounced\n",
            "back strongly from the effects of the deadly Sars virus in 2003.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "trC25DXuE-uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "t1=word_tokenize(sum_)\n",
        "t2=word_tokenize(sum_tfidf)\n",
        "t3=word_tokenize(sum_cos)\n",
        "t4=word_tokenize(sum_kmean)\n",
        "print(\"Bleu score of tfidf model: \",sentence_bleu(t1,t2,weights=(1,0,0,0)))\n",
        "print(\"Bleu score of cosine similarity model: \",sentence_bleu(t1,t3,weights=(1,0,0,0)))\n",
        "print(\"Bleu score of kmeans clustering model: \",sentence_bleu(t1,t4,weights=(1,0,0,0)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2K4M8N92WRt",
        "outputId": "2a6c30b7-9b4e-46e8-ee7f-adc78aa6c66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu score of tfidf model:  0.047619047619047616\n",
            "Bleu score of cosine similarity model:  0.04807692307692307\n",
            "Bleu score of kmeans clustering model:  0.03921568627450981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial import distance\n",
        "import textwrap\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Load NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# PDF Parsing\n",
        "def parse_pdf(pdf_path):\n",
        "    text_by_page = []\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text_by_page.append(page.extract_text())\n",
        "    return text_by_page\n",
        "\n",
        "# Your Summarization Algorithm\n",
        "def summarize_page(text):\n",
        "    # Your summarization algorithm here\n",
        "    # ... (include the code for your summarization algorithm)\n",
        "\n",
        "# Main Process\n",
        "def main(pdf_path):\n",
        "    text_by_page = parse_pdf(pdf_path)\n",
        "    reference_summaries = [\n",
        "        \"Reference summary for page 1.\",\n",
        "        \"Reference summary for page 2.\",\n",
        "        \"Reference summary for page 3.\",\n",
        "        # Add more reference summaries for each page\n",
        "    ]\n",
        "    for page_num, (page_text, reference_summary) in enumerate(zip(text_by_page, reference_summaries)):\n",
        "        generated_summary = summarize_page(page_text)\n",
        "        bleu_score = sentence_bleu([reference_summary], generated_summary)\n",
        "        print(f\"Page {page_num + 1} BLEU Score: {bleu_score:.4f}\")\n",
        "        print(\"Generated Summary:\", generated_summary)\n",
        "        print(\"Reference Summary:\", reference_summary)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "# Replace with your PDF file path\n",
        "pdf_file_path = 'path_to_your_pdf.pdf'\n",
        "main(pdf_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "4yHR20tFE_p-",
        "outputId": "cdb791b3-6cae-485d-a262-0b2bfd841417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-0fb8acc87919>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    def main(pdf_path):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "3y23142wpEY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a152d7a-43e5-439a-af4a-b38783aef9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/232.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n"
      ],
      "metadata": {
        "id": "jgv8_gqgtSIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "breaking pdf and calculating scores\n"
      ],
      "metadata": {
        "id": "IlhFvgJAk3xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Load NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def parse_pdf(pdf_path):\n",
        "    text_by_page = []\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text_by_page.append(page.extract_text())\n",
        "    return text_by_page\n",
        "\n",
        "\n",
        "def summarize_page(text):\n",
        "    # Preprocess the input text\n",
        "    sentence = nltk.sent_tokenize(text)\n",
        "    corpus = []\n",
        "    for i in range(len(sentence)):\n",
        "        sen = re.sub('[^a-zA-Z]', \" \", sentence[i])\n",
        "        sen = sen.lower()\n",
        "        sen = sen.split()\n",
        "        sen = ' '.join([i for i in sen if i not in stopwords.words('english')])\n",
        "        corpus.append(sen)\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    all_words = [i.split() for i in corpus]\n",
        "    model = Word2Vec(all_words, min_count=1, vector_size=300)\n",
        "\n",
        "    # Generate sentence vectors using Word2Vec\n",
        "    sent_vector = []\n",
        "    for i in corpus:\n",
        "        plus = 0\n",
        "        for j in i.split():\n",
        "            plus += model.wv[j]\n",
        "        plus = plus / len(i.split())\n",
        "        sent_vector.append(plus)\n",
        "\n",
        "    # Apply K-Means clustering\n",
        "    n_clusters = 5\n",
        "    kmeans = KMeans(n_clusters, init='k-means++', random_state=42)\n",
        "    y_kmeans = kmeans.fit_predict(sent_vector)\n",
        "\n",
        "    my_list = []\n",
        "    for i in range(n_clusters):\n",
        "        my_dict = {}\n",
        "        for j in range(len(y_kmeans)):\n",
        "            if y_kmeans[j] == i:\n",
        "                my_dict[j] = distance.euclidean(kmeans.cluster_centers_[i], sent_vector[j])\n",
        "        min_distance = min(my_dict.values())\n",
        "        my_list.append(min(my_dict, key=my_dict.get))\n",
        "\n",
        "    # Generate the final summary using selected sentences\n",
        "    generated_summary = \"\"\n",
        "    for i in sorted(my_list):\n",
        "        generated_summary += sentence[i]\n",
        "\n",
        "    return generated_summary\n",
        "\n",
        "# Main Process\n",
        "def main(pdf_path):\n",
        "    text_by_page = parse_pdf(pdf_path)\n",
        "    reference_summaries = [\"Seldom does saturn and Morae,juxtapose and mate in the great fabric of Gaea, as it did to rekindle my thread of life with her;## the writer here wanted to stress the great luck and the timing of it, involved on this planet, that ## morae -goddess of fate , saturn god of time,gaea-goddess of earthunderlined , his rekindling contact with the protagonist.\",\n",
        "\"In this cradle of desolation reeking of grim profound, she bounced around,emanating a pervading tang of joy,boundless and intense;## in this world of endless despair and grim, the protagonist had somehow found a way to thrive and bounced around sharing and inducing this infective spirit on all the souls she touched, and influenced. Her presence created an endless joy which was anything but temporary as she changed the very worldview of people she touched.\",\n",
        "        \"##lying underneath her visible grace and poise duly complemented with her calm and joy Lay a scarred and equally humane existence(represented by a ship wreck) with her share of loss , pain and regrets, which she had buried deep down her bubbly self.\"\n",
        "\n",
        "\n",
        "        # Add more reference summaries for each page\n",
        "    ]\n",
        "    for page_num, (page_text, reference_summary) in enumerate(zip(text_by_page, reference_summaries)):\n",
        "        generated_summary = summarize_page(page_text)\n",
        "        bleu_score = sentence_bleu([reference_summary], generated_summary)\n",
        "        print(f\"Page {page_num + 1} BLEU Score: {bleu_score:.4f}\")\n",
        "        print(\"Generated Summary:\", generated_summary)\n",
        "        print(\"Reference Summary:\", reference_summary)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "# Replace with your PDF file path\n",
        "pdf_file_path = '/content/drive/MyDrive/POEM_EXPLANATION.pdf'\n",
        "main(pdf_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYvl37lPk8EB",
        "outputId": "01bd31e6-aa79-4308-8ec4-c878b9ea6859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 BLEU Score: 0.0179\n",
            "Generated Summary: SHE\n",
            ":\n",
            "A\n",
            "POR TRAIT\n",
            "S e l d o m\n",
            "d o e s\n",
            "s a t u r n\n",
            "a n d\n",
            "M o r a e\n",
            ", j u x t a p o s e\n",
            "a n d\n",
            "m a t e\n",
            "i n\n",
            "t h e\n",
            "g r e a t\n",
            "f a b r i c\n",
            "o f\n",
            "G a e a\n",
            ",\n",
            "a s\n",
            "i t\n",
            "d i d\n",
            "t o\n",
            "r e k i n d l e\n",
            "m y\n",
            "t h r e a d\n",
            "o f\n",
            "l i f e\n",
            "w i t h\n",
            "h e r ;\n",
            "# #\n",
            "t h e\n",
            "w r i t e r\n",
            "h e r e\n",
            "w a n t e d\n",
            "t o\n",
            "s t r e s s\n",
            "t h e\n",
            "g r e a t\n",
            "l u c k\n",
            "a n d\n",
            "t h e\n",
            "t i m i n g\n",
            "o f\n",
            "i t ,\n",
            "i n v o l v e d\n",
            "o n\n",
            "t h i s\n",
            "p l a n e t ,\n",
            "t h a t\n",
            "# #\n",
            "m o r a e\n",
            "- g o d d e s s\n",
            "o f\n",
            "f a t e\n",
            ",\n",
            "s a t u r n\n",
            "g o d\n",
            "o f\n",
            "t i m e , g a e a - g o d d e s s\n",
            "o f\n",
            "e a r t h\n",
            "u n d e r l i n e d\n",
            ",\n",
            "h i s\n",
            "r e k i n d l i n g\n",
            "c o n t a c t\n",
            "w i t h\n",
            "t h e\n",
            "p r o t a g o n i s t .F o r\n",
            "m y\n",
            "d a m p\n",
            ", p a l e\n",
            "c o a l s\n",
            "o f\n",
            "e x u b e r a n c e\n",
            "a n d\n",
            "h a l e , b u r s t\n",
            "i n t o\n",
            "f l a m e s\n",
            "o f\n",
            "b o n h o m i e , p r e v i o u s l y\n",
            "u n c h a r t e d , u p o n\n",
            "h e r\n",
            "a b r u p t , t e n d e r\n",
            "e m b r a c e\n",
            "o f\n",
            "m y\n",
            "a r i d\n",
            "l a n d s c a p e\n",
            "o f\n",
            "m i n e ,\n",
            "m e a g e r , s i c k l y\n",
            "y e t\n",
            "a s p i r a t i o n a l\n",
            "e x i s t e n c e\n",
            "o n\n",
            "t h e\n",
            "o r c h a r d\n",
            "o f\n",
            "l i f e , s a t i a t e d ,\n",
            "y e t\n",
            "p e r t u r b e d\n",
            "b y\n",
            "t h e\n",
            "h i s t o r y\n",
            "a n d\n",
            "v i l i f y i n g\n",
            "u n a b a s h e d l y\n",
            "t h e\n",
            "a p p r o a c h i n g\n",
            "s a n d s\n",
            "o f\n",
            "t i m e ;\n",
            "# # #\n",
            "h e r e\n",
            "t h e\n",
            "w r i t e r\n",
            "w a n t e d\n",
            "t o\n",
            "b r i n g\n",
            "o u t\n",
            "t h a t\n",
            "h i s\n",
            "p a l e\n",
            ", s u n d r y\n",
            "a n d\n",
            "l i f e l e s s\n",
            "e x i s t e n c e ,\n",
            "d e v o i d\n",
            "o f\n",
            "h a l e\n",
            "o r\n",
            "h a p p i n e s s\n",
            "w a s\n",
            "r e i g n i t e d\n",
            "b y\n",
            "t h e\n",
            "p r o t a g o n i s t ’ s\n",
            "a r r i v a l\n",
            "a n d\n",
            "i n d i s c r i m i n a t e\n",
            "e m b r a c e\n",
            "a n d\n",
            "h o w\n",
            "h i s\n",
            "m i s e r a b l e\n",
            "y e t\n",
            "a m b i t i o u s\n",
            "a n d\n",
            "a s p i r i n g\n",
            "l i f e\n",
            "a n d\n",
            "w o r l d v i e w ,\n",
            "t h o u g h\n",
            "f u l f i l l e d\n",
            "a n d\n",
            "m a n a g e a b l e\n",
            "s o m e w h a t\n",
            "y e t\n",
            "s t i l l\n",
            "m o r t i f i e d\n",
            "b y\n",
            "h i s\n",
            "h i s t o r y\n",
            "o f\n",
            "e m o t i o n a l\n",
            "b a g g a g e\n",
            "a n d\n",
            "g a p i n g\n",
            "b l a n k l y\n",
            "o u t\n",
            "v i l l i f i e d\n",
            "a t\n",
            "a\n",
            "u n c e r t a i n\n",
            "f u t u r e\n",
            "a n d\n",
            "t i m e\n",
            "S o l i t u d e , s u l k\n",
            "m y\n",
            "w o r t h y\n",
            "c o m p a n i o n s\n",
            ", w e\n",
            "s t a r e d\n",
            "i n t o\n",
            "o b l i v i o n\n",
            "o f\n",
            "m y s t e r y\n",
            "a n d\n",
            "t i m e , f l a n k e d\n",
            "b y\n",
            "E r i s\n",
            "’ i m p a s s i o n e d\n",
            "s m i r k ,\n",
            "w h e n\n",
            "S H E\n",
            "a r r i v e d .# #\n",
            "b e i n g\n",
            "i n\n",
            "s o l i t u d e\n",
            "a n d\n",
            "s u l k i n g ,\n",
            "f r e q u e n t l y\n",
            "b e i n g\n",
            "h i s\n",
            "s t a t e\n",
            "o f\n",
            "m i n d ,\n",
            "n o t\n",
            "e x t e r n a l l y ,\n",
            "b u t\n",
            "i n t e r n a l l y\n",
            "d e e p\n",
            "d o w n .( E r i s\n",
            "- g o d d e s s\n",
            "o f\n",
            "s t r i f e\n",
            "a n d\n",
            "d i s c o r d ) .H e r e\n",
            "t h e\n",
            "w r i t e r\n",
            "w a s\n",
            "c o n t i n u a l l y\n",
            "i n\n",
            "a\n",
            "s t a t e\n",
            "o f\n",
            "c o n s t a n t l y\n",
            "j u g g l i n g\n",
            "b e t w e e n\n",
            "h i s\n",
            "t w o\n",
            "e x t r e m e s ,\n",
            "m u c h\n",
            "t o\n",
            "e r i s ’\n",
            "d e l i g h t\n",
            ",\n",
            "j u s t\n",
            "w h e n\n",
            "t h e\n",
            "p r o t a g o n i s t\n",
            "a r r i v e d .\n",
            "Reference Summary: Seldom does saturn and Morae,juxtapose and mate in the great fabric of Gaea, as it did to rekindle my thread of life with her;## the writer here wanted to stress the great luck and the timing of it, involved on this planet, that ## morae -goddess of fate , saturn god of time,gaea-goddess of earthunderlined , his rekindling contact with the protagonist.\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 2 BLEU Score: 0.0154\n",
            "Generated Summary: I n\n",
            "t h i s\n",
            "c r a d l e\n",
            "o f\n",
            "d e s o l a t i o n\n",
            "r e e k i n g\n",
            "o f\n",
            "g r i m\n",
            "p r o f o u n d ,\n",
            "s h e\n",
            "b o u n c e d\n",
            "a r o u n d , e m a n a t i n g\n",
            "a\n",
            "p e r v a d i n g\n",
            "t a n g\n",
            "o f\n",
            "j o y , b o u n d l e s s\n",
            "a n d\n",
            "i n t e n s e ;\n",
            "# #\n",
            "i n\n",
            "t h i s\n",
            "w o r l d\n",
            "o f\n",
            "e n d l e s s\n",
            "d e s p a i r\n",
            "a n d\n",
            "g r i m ,\n",
            "t h e\n",
            "p r o t a g o n i s t\n",
            "h a d\n",
            "s o m e h o w\n",
            "f o u n d\n",
            "a\n",
            "w a y\n",
            "t o\n",
            "t h r i v e\n",
            "a n d\n",
            "b o u n c e d\n",
            "a r o u n d\n",
            "s h a r i n g\n",
            "a n d\n",
            "i n d u c i n g\n",
            "t h i s\n",
            "i n f e c t i v e\n",
            "s p i r i t\n",
            "o n\n",
            "a l l\n",
            "t h e\n",
            "s o u l s\n",
            "s h e\n",
            "t o u c h e d ,\n",
            "a n d\n",
            "i n f l u e n c e d .H e r\n",
            "p r e s e n c e\n",
            "c r e a t e d\n",
            "a n\n",
            "e n d l e s s\n",
            "j o y\n",
            "w h i c h\n",
            "w a s\n",
            "a n y t h i n g\n",
            "b u t\n",
            "t e m p o r a r y\n",
            "a s\n",
            "s h e\n",
            "c h a n g e d\n",
            "t h e\n",
            "v e r y\n",
            "w o r l d v i e w\n",
            "o f\n",
            "p e o p l e\n",
            "s h e\n",
            "t o u c h e d\n",
            "S h e\n",
            "s w i r l e d , d a n c e d\n",
            "i n\n",
            "t h i s\n",
            "l i f e l e s s , s a n g u i n e\n",
            "c e m e t e r y\n",
            "o f\n",
            "h o p e s\n",
            "a n d\n",
            "q u a s h e d\n",
            "a s p i r a t i o n s ,\n",
            "r e d e e m i n g\n",
            "a n d\n",
            "e n r i c h i n g\n",
            "s o i l\n",
            "o f\n",
            "d e s p a i r , r e e k i n g\n",
            "e m a s c u l a t e d\n",
            "z e s t ,\n",
            "i n t o\n",
            "a\n",
            "t h o r o u g h b r e d\n",
            "f i e l d\n",
            "o f\n",
            "E l y s i u m , h e r\n",
            "m a g i c\n",
            "p a l i n g\n",
            "t h e\n",
            "g o l d e n\n",
            "f l e e c e ;\n",
            "# #\n",
            "a s\n",
            "s h e\n",
            "d a n c e d\n",
            "a n d\n",
            "t h r i v e d\n",
            "i n\n",
            "t h i s\n",
            "w o r l d\n",
            "f u l l\n",
            "o f\n",
            "d e a d\n",
            "d r e a m s\n",
            ",\n",
            "h o p e s\n",
            "a n d\n",
            "b r o k e n\n",
            "a s p i r a t i o n s ,\n",
            "P l o w i n g\n",
            "t h i s ,\n",
            "s o i l\n",
            "d e v o i d\n",
            "o f\n",
            "a n y\n",
            "z e s t\n",
            "a n d\n",
            "j o y ,\n",
            "i n t o\n",
            "E l y s i u m (\n",
            "f i e l d\n",
            "o f\n",
            "e l y s i u m\n",
            "w h e r e\n",
            "o n l y\n",
            "b l e s s e d\n",
            "s o u l d\n",
            "c o u l d\n",
            "g o\n",
            "i n\n",
            "t h e\n",
            "a f t e r l i f e ) f i e l d s .G o l d e n\n",
            "f l e e c e ( a\n",
            "v a l u a b l e\n",
            "f l e e c e\n",
            "o f\n",
            "s h e e p\n",
            "i n\n",
            "m y t h o l o g y\n",
            "w h i c h\n",
            "c o u l d\n",
            "c u r e\n",
            "d e c a y\n",
            ", d i s e a s e\n",
            "o f\n",
            "a n y\n",
            "k i n d\n",
            "a n d\n",
            "s o m e t i m e s\n",
            "e v e n\n",
            "b r i n g\n",
            "b a c k\n",
            "l i f e\n",
            "t o\n",
            "a\n",
            "l i f e l e s s\n",
            "t h i n g )\n",
            "B e a r i n g\n",
            "a\n",
            "c h a r m\n",
            "o f\n",
            "w o r d s\n",
            "u n m a t c h e d , s h e\n",
            "s e t\n",
            "d o w n\n",
            "t i l l i n g\n",
            "t h e\n",
            "l a n d\n",
            ",\n",
            "h e r\n",
            "s w e e t\n",
            "b e a d s\n",
            "o f\n",
            "s w e a t\n",
            ".t r i c k l i n g\n",
            "d o w n\n",
            "h e r\n",
            "c u r v a c e o u s\n",
            "g o r g e , i n t o\n",
            "a\n",
            "u n q u e n c h e d\n",
            "b e d , f a m i s h e d\n",
            "o f\n",
            "f e m i n i n e\n",
            "g r a c e ;\n",
            "# # w i t h\n",
            "h e r\n",
            "i n c o m p a r a b l e\n",
            "a r t i c u l a t i o n ,\n",
            "h e r\n",
            "s w e e t\n",
            "i m p a t i e n c e\n",
            "m a t c h e d\n",
            "o n l y\n",
            "b y\n",
            "h e r\n",
            "s t r i k i n g\n",
            "b e a u t y\n",
            "S h e\n",
            "g o t\n",
            "t o\n",
            "w o r k\n",
            "p u t t i n g\n",
            "l i f e\n",
            "i n t o\n",
            "a\n",
            "f i e l d\n",
            "d e v o i d\n",
            "o f\n",
            "h e r\n",
            "f e m i n i n e\n",
            "g r a c e\n",
            "a n d\n",
            "z e s t\n",
            "H e r\n",
            "e l i x i r\n",
            "s m i l e\n",
            "e n t h r a l l i n g\n",
            ",\n",
            "r e k i n d l i n g\n",
            "a\n",
            "b r e w\n",
            "o f\n",
            "p a s s i o n , u n r e q u i t e d\n",
            ", l o n g i n g\n",
            "a b a t e m e n t\n",
            ";\n",
            "H e r\n",
            "g a i t\n",
            "n o n c h a l a n t , s h e\n",
            "m e a n d e r e d\n",
            ", s w a y e d\n",
            ",\n",
            "u n m i n d f u l\n",
            "a n d\n",
            "o b l i v i o u s\n",
            "t o\n",
            "t h e\n",
            "f a c i l e\n",
            "r a g i n g\n",
            "H E R\n",
            "p r e s e n c e\n",
            "c o n t r i v e d .# # a s\n",
            "c a l m , c o m p o s e d\n",
            "a n d\n",
            "s o r t e d\n",
            "t h e\n",
            "p r o t a g o n i s t\n",
            "l o o k e d\n",
            "e x t e r n a l l y\n",
            "a n d\n",
            "t o\n",
            "t h e\n",
            "w i d e\n",
            "w o r l d\n",
            "L o o k i n g\n",
            "b e y o n d\n",
            "t h e\n",
            "c o n s p i c u o u s\n",
            "l a y e r\n",
            "o f\n",
            "g r a c e , b r a n d i s h e d\n",
            "w i t h\n",
            "p o i s e\n",
            "b e a r i n g\n",
            "u n d u l a t i n g\n",
            "r i p p l e s\n",
            "o f\n",
            "c a l m\n",
            "a n d\n",
            "t w i n k l e , l a y\n",
            "s c a r r e d\n",
            "s h i p w r e c k , s o r e\n",
            "w i t h\n",
            "l o s s\n",
            "a n d\n",
            "a n g u i s h , i t s\n",
            "m u s h y\n",
            "Reference Summary: In this cradle of desolation reeking of grim profound, she bounced around,emanating a pervading tang of joy,boundless and intense;## in this world of endless despair and grim, the protagonist had somehow found a way to thrive and bounced around sharing and inducing this infective spirit on all the souls she touched, and influenced. Her presence created an endless joy which was anything but temporary as she changed the very worldview of people she touched.\n",
            "------------------------------\n",
            "Page 3 BLEU Score: 0.0127\n",
            "Generated Summary: # # l y i n g\n",
            "u n d e r n e a t h\n",
            "h e r\n",
            "v i s i b l e\n",
            "g r a c e\n",
            "a n d\n",
            "p o i s e\n",
            "d u l y\n",
            "c o m p l e m e n t e d\n",
            "w i t h\n",
            "h e r\n",
            "c a l m\n",
            "a n d\n",
            "j o y\n",
            "L a y\n",
            "a\n",
            "s c a r r e d\n",
            "a n d\n",
            "e q u a l l y\n",
            "h u m a n e\n",
            "e x i s t e n c e ( r e p r e s e n t e d\n",
            "b y\n",
            "a\n",
            "s h i p\n",
            "w r e c k )\n",
            "w i t h\n",
            "h e r\n",
            "s h a r e\n",
            "o f\n",
            "l o s s\n",
            ",\n",
            "p a i n\n",
            "a n d\n",
            "r e g r e t s ,\n",
            "w h i c h\n",
            "s h e\n",
            "h a d\n",
            "b u r i e d\n",
            "d e e p\n",
            "d o w n\n",
            "h e r\n",
            "b u b b l y\n",
            "s e l f\n",
            "o u t g r o w t h\n",
            "c r y i n g\n",
            "u n r e c i p r o c a t e d\n",
            "w a r m t h\n",
            "t h a t\n",
            "n e v e r\n",
            "w a s , i t s\n",
            "f o r e m a s t , p r i s t i n e\n",
            "a n d\n",
            "# # # h e r\n",
            "m o i s t\n",
            "e y e s , a s\n",
            "s h e\n",
            "r e c o l l e c t e d\n",
            "h e r\n",
            "u n r e q u i t e d\n",
            "a f f e c t i o n\n",
            "m a j e s t i c , b e g u i l i n g\n",
            "s h r o u d s\n",
            "o f\n",
            "s i n k i n g\n",
            "G r a v i t y\n",
            "o f\n",
            "u n c e r t a i n t y , b e r e f t\n",
            "o f\n",
            "a s s u r a n c e\n",
            "o f\n",
            "p r o m i s e ;\n",
            "I t s\n",
            "s o f t e n e d , b r i t t l e\n",
            "d e c k , r e e l i n g\n",
            "u n d e r\n",
            "c u r r e n t s\n",
            "o f\n",
            "b e t r a y a l\n",
            "a n d\n",
            "a g i t a t i o n\n",
            "# #\n",
            "d e e p\n",
            "b e l o w\n",
            "h e r\n",
            "m a j e s t i c\n",
            "c o n f i d e n t\n",
            "s e l f ,\n",
            "l a y\n",
            "d e e p\n",
            "d o u b t s\n",
            "a n d\n",
            "u n c e r t a i n t y ,\n",
            "t h e\n",
            "g r a v i t y\n",
            "o f\n",
            "w h o m\n",
            "c o n t i n u e\n",
            "t o\n",
            "r e s t r a i n\n",
            "a n d\n",
            "h i n g e\n",
            "h e r\n",
            "b a c k ,\n",
            "s i n k i n g\n",
            "h e r\n",
            "i n t o\n",
            "d o u b t\n",
            ",\n",
            "b e r e f t\n",
            "o f\n",
            "p r o m i s e\n",
            "a n d\n",
            "h e r\n",
            "w o r t h .# #\n",
            "u p o n\n",
            "d i s c o v e r i n g\n",
            "d e e p e r\n",
            "t h e\n",
            "w r i t e r\n",
            "c a m e\n",
            "a c r o s s\n",
            "a\n",
            "w o m a n\n",
            "o f\n",
            "s t r i k i n g l y\n",
            "r e s e m b l a n c e\n",
            "t o\n",
            "h e r\n",
            ",\n",
            "u n t o u c h e d\n",
            "b y\n",
            "m e r e n\n",
            "m o r t a l i t y\n",
            "a n d\n",
            "b o u n d a t i o n s\n",
            "o f\n",
            "t i m e ,\n",
            "r a d i a t i n g\n",
            "a n d\n",
            "i n s p i r i n g\n",
            "w i t h\n",
            "h e r\n",
            "p e a c e ,\n",
            "c a l m\n",
            ",\n",
            "w i s d o m\n",
            "a n d\n",
            "c o m p o s u r e ,\n",
            "e s t a b l i s h e d\n",
            "s t r o n g l y\n",
            "d o w n\n",
            "i n\n",
            "h e r\n",
            "m e m o r y ,\n",
            "a\n",
            "s o u r c e , p e r h a p s\n",
            "o f\n",
            "h e r\n",
            "s t r e n g t h\n",
            "a n d\n",
            "p o w e r ,\n",
            "t h e\n",
            "p r o t a g o n i s t ’ s\n",
            "m o t h e r .A s\n",
            "i\n",
            "e m e r g e d\n",
            ", s i g h i n g\n",
            "f o r\n",
            "w a r m t h , I\n",
            "w a s\n",
            "g r e e t e d\n",
            "w i t h\n",
            "a\n",
            "m o s t\n",
            "h e a r t y\n",
            "s m i l e , a d o r n e d\n",
            "p r e c a r i o u s l y\n",
            "W i t h\n",
            "a\n",
            "p a i r\n",
            "o f\n",
            "s a l i n e\n",
            "P e a r l s , b e g g i n g\n",
            "t o\n",
            "b e\n",
            "R e l i n q u i s h e d .# #\n",
            "a s\n",
            "t h e\n",
            "w r i t e r\n",
            "c a m e\n",
            "t o\n",
            "t e r m s\n",
            "w i t h\n",
            "t h e\n",
            "p r o t a g o n i s t ’ s\n",
            "h i s t o r y\n",
            "a n d\n",
            "d e e p\n",
            "d o w n\n",
            "m e m o r i e s ,\n",
            "h e\n",
            "w a s\n",
            "a s t o n i s h e d\n",
            "a n d\n",
            "h o r r i f i e d ,\n",
            "y e t\n",
            "d e s p i t e\n",
            "a l l\n",
            "t h i s ,\n",
            "s h e\n",
            "d i s p l a y e d\n",
            "n o t\n",
            "a\n",
            "s i g n\n",
            "o f\n",
            "f r a i l t y\n",
            "o r\n",
            "s c a r r e d n e s s ,\n",
            "b u t\n",
            "r a t h e r\n",
            "h a d\n",
            "a\n",
            "l o n g\n",
            "c h e e r f u l ,\n",
            "a l l\n",
            "e n c o m p a s s i n g\n",
            "s m i l e ,\n",
            "w i t h\n",
            "h e r\n",
            "t e a r s\n",
            "a n d\n",
            "p a i n ,\n",
            "b e g g i n g\n",
            "r e l e a s e ,\n",
            "a s\n",
            "s h e\n",
            "p r o c e s s e d\n",
            "a l l\n",
            "o f\n",
            "t h a t\n",
            ",\n",
            "y e t\n",
            "s t a y e d\n",
            "s t r o n g\n",
            "a n d\n",
            "s t e a d y\n",
            "A s\n",
            "o u r\n",
            "e y e s\n",
            "l o c k e d , I\n",
            "w a s\n",
            "s u b m e r g e d\n",
            "h e a d l o n g\n",
            "i n t o\n",
            "a n\n",
            "i n f i n i t e\n",
            "V o r t e x\n",
            "o f\n",
            "d e p t h\n",
            "a n d\n",
            "b l i s s , S H E\n",
            "c o n j u r e d .T h e n c e\n",
            "i\n",
            "w i s h e d\n",
            "f o r\n",
            "H a d e s ’\n",
            "e m b r a c e\n",
            ".\n",
            "Reference Summary: ##lying underneath her visible grace and poise duly complemented with her calm and joy Lay a scarred and equally humane existence(represented by a ship wreck) with her share of loss , pain and regrets, which she had buried deep down her bubbly self.\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}